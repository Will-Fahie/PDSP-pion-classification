{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# MLP Pion Classifier — Summary Statistics\n",
    "\n",
    "Standalone MLP trained on four track-level summary statistics:\n",
    "- `chi²/ndof_proton` — chi-squared per degree of freedom under the proton Bethe-Bloch hypothesis\n",
    "- `track_length` — total track length (cm)\n",
    "- `track_score` — Pandora track quality score\n",
    "- `dEdX_median` — median energy loss per unit length (computed from hit-level sequence)\n",
    "\n",
    "This is the **contemporary method** baseline for comparison with the hit-level CNN and hybrid models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Run from project root regardless of where the notebook is opened from\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "_utils = os.path.join(os.getcwd(), 'utils')\n",
    "if _utils not in sys.path:\n",
    "    sys.path.insert(0, _utils)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from general_utils import purity, efficiency, create_confusion_matrix\n",
    "from evaluation_utils import (\n",
    "    plot_training_curves,\n",
    "    optimise_threshold,\n",
    "    plot_roc_and_purity_efficiency,\n",
    "    plot_confusion_matrix,\n",
    "    save_results,\n",
    ")\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 13,657 pions / 50,000 (27.3%)\n",
      "Test:  15,245 pions / 55,815 (27.3%)\n",
      "summary: (50000, 4)\n"
     ]
    }
   ],
   "source": [
    "data_size = \"50000\"  # \"50000\" or \"all\"\n",
    "all_summary_stats = True\n",
    "\n",
    "with open(f\"prepared-data/train_{data_size}.pkl\", \"rb\") as f:\n",
    "    train = pickle.load(f)\n",
    "with open(\"prepared-data/test.pkl\", \"rb\") as f:\n",
    "    test = pickle.load(f)\n",
    "\n",
    "y_train = train[\"labels\"].astype(int)\n",
    "y_test  = test[\"labels\"].astype(int)\n",
    "\n",
    "if all_summary_stats:\n",
    "    summ_train_n = train[\"summary\"]          # (N, 4): chi2, length, score, dEdX_median\n",
    "    summ_test_n  = test[\"summary\"]\n",
    "else:\n",
    "    summ_train_n = train[\"summary\"][:, [1, 3]]   # track_length, dEdX_median\n",
    "    summ_test_n  = test[\"summary\"][:, [1, 3]]\n",
    "\n",
    "num_features = summ_train_n.shape[1]\n",
    "print(f\"Train: {y_train.sum():,} pions / {len(y_train):,} ({100*y_train.mean():.1f}%)\")\n",
    "print(f\"Test:  {y_test.sum():,} pions / {len(y_test):,} ({100*y_test.mean():.1f}%)\")\n",
    "print(f\"summary: {summ_train_n.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 196, Test batches: 219\n"
     ]
    }
   ],
   "source": [
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, summary, labels):\n",
    "        self.summary = torch.FloatTensor(summary)\n",
    "        self.labels  = torch.FloatTensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.summary[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "train_dataset = SummaryDataset(summ_train_n, y_train)\n",
    "test_dataset  = SummaryDataset(summ_test_n,  y_test)\n",
    "\n",
    "class_counts   = np.bincount(y_train)\n",
    "sample_weights = (1.0 / class_counts)[y_train]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "BATCH_SIZE   = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters: 801\n"
     ]
    }
   ],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, n_features=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 32), nn.BatchNorm1d(32), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(32, 16),         nn.BatchNorm1d(16), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce   = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        pt    = torch.exp(-bce)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * bce\n",
    "        return focal.mean()\n",
    "\n",
    "\n",
    "model = MLPClassifier(n_features=num_features).to(device)\n",
    "print(f\"\\nParameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 | Train: 0.1454 | Val: 0.1281 | Purity: 55.9% | Efficiency: 80.0%\n",
      "Epoch  10 | Train: 0.1219 | Val: 0.1176 | Purity: 55.9% | Efficiency: 82.3%\n",
      "Epoch  20 | Train: 0.1175 | Val: 0.1150 | Purity: 56.6% | Efficiency: 82.7%\n",
      "Epoch  30 | Train: 0.1161 | Val: 0.1139 | Purity: 58.8% | Efficiency: 80.5%\n",
      "Early stopping at epoch 36\n",
      "\n",
      "Best validation loss: 0.1134\n"
     ]
    }
   ],
   "source": [
    "criterion = FocalLoss(gamma=2.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "N_EPOCHS            = 80\n",
    "EARLY_STOP_PATIENCE = 15\n",
    "\n",
    "best_val_loss    = float('inf')\n",
    "patience_counter = 0\n",
    "best_state       = None\n",
    "history = {'train_loss': [], 'val_loss': [], 'purity': [], 'efficiency': []}\n",
    "\n",
    "\n",
    "def train_one_epoch():\n",
    "    model.train()\n",
    "    total_loss, n = 0, 0\n",
    "    for summ, y in train_loader:\n",
    "        summ, y = summ.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(summ), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(y)\n",
    "        n += len(y)\n",
    "    return total_loss / n\n",
    "\n",
    "\n",
    "def evaluate(threshold=0.5):\n",
    "    model.eval()\n",
    "    total_loss, n = 0, 0\n",
    "    all_probs, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for summ, y in test_loader:\n",
    "            summ, y = summ.to(device), y.to(device)\n",
    "            logits = model(summ)\n",
    "            total_loss += criterion(logits, y).item() * len(y)\n",
    "            n += len(y)\n",
    "            all_probs.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy().astype(int))\n",
    "    all_probs  = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    preds = (all_probs >= threshold).astype(int)\n",
    "    pur   = purity(preds, all_labels, [1], [1])\n",
    "    eff   = efficiency(preds, all_labels, [1], [1])\n",
    "    return total_loss / n, pur, eff, all_probs, all_labels\n",
    "\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train_one_epoch()\n",
    "    val_loss, pur, eff, _, _ = evaluate()\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['purity'].append(pur)\n",
    "    history['efficiency'].append(eff)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | \"\n",
    "              f\"Purity: {100*pur:.1f}% | Efficiency: {100*eff:.1f}%\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss    = val_loss\n",
    "        patience_counter = 0\n",
    "        best_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "print(f\"\\nBest validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_curves(history, 'MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Threshold Optimisation & Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, _, test_probs, test_labels = evaluate()\n",
    "\n",
    "best_threshold = optimise_threshold(test_probs, test_labels, label='MLP', color='forestgreen')\n",
    "final_preds = (test_probs >= best_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_and_purity_efficiency([\n",
    "    {'probs': test_probs, 'labels': test_labels, 'threshold': best_threshold,\n",
    "     'color': 'forestgreen', 'label': 'MLP'}\n",
    "], title='MLP \\u2014 chi\\u00b2/ndof_p + track length + score + dEdX median')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, final_preds, best_threshold, title='MLP Pion Classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = save_results(\n",
    "    test_probs, test_labels, best_threshold,\n",
    "    model_name=\"MLP (chi\\u00b2/ndof_p + track length + score + dEdX median)\",\n",
    "    save_path=\"results/mlp_summary.pkl\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
