{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# Run from project root regardless of where the notebook is opened from\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "PION_CLASS   = '$\\\\pi^{\\\\pm}$'\n",
    "DEDX_CLIP    = 50.0\n",
    "RR_CLIP      = 150.0\n",
    "CHI2_CLIP    = 500.0\n",
    "LEN_CLIP     = 300.0\n",
    "MAX_LEN      = 120\n",
    "TRAIN_50K    = 50_000\n",
    "TEST_SIZE    = 0.2\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 279,071 tracks\n"
     ]
    }
   ],
   "source": [
    "with open(\"extracted-data/ALL_DATA.pkl\", \"rb\") as f:\n",
    "    raw = pickle.load(f)\n",
    "print(f\"Loaded {len(raw):,} tracks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: (279071, 120, 2), summary: (279071, 4), masks: (279071, 120)\n",
      "Pions: 76,224 / 279,071 (27.3%)\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1 if PION_CLASS in d['particle'] else 0 for d in raw], dtype=np.float32)\n",
    "\n",
    "sequences = np.stack([\n",
    "    np.stack([d['dEdX_sequence'], d['residual_range_sequence']], axis=-1)\n",
    "    for d in raw\n",
    "], axis=0).astype(np.float32)\n",
    "\n",
    "# Always build 4-feature summary; classifiers choose 2 or 4 via all_summary_stats\n",
    "summary = np.array([\n",
    "    [d['track_chi2/ndof_proton'], d['track_length'], d['track_score'], d['dEdX_median']]\n",
    "    for d in raw\n",
    "], dtype=np.float32)\n",
    "\n",
    "seq_lengths = np.array([d['sequence_length'] for d in raw])\n",
    "masks = np.zeros((len(raw), MAX_LEN), dtype=np.float32)\n",
    "for i, L in enumerate(seq_lengths):\n",
    "    masks[i, :min(L, MAX_LEN)] = 1.0\n",
    "\n",
    "print(f\"sequences: {sequences.shape}, summary: {summary.shape}, masks: {masks.shape}\")\n",
    "print(f\"Pions: {labels.sum():,.0f} / {len(labels):,} ({100*labels.mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 60,979 pions / 223,256 (27.3%)\n",
      "Test:  15,245 pions / 55,815 (27.3%)\n"
     ]
    }
   ],
   "source": [
    "(seq_train_all, seq_test,\n",
    " summ_train_all, summ_test,\n",
    " mask_train_all, mask_test,\n",
    " y_train_all, y_test,\n",
    " idx_train_all, idx_test) = train_test_split(\n",
    "    sequences, summary, masks, labels, np.arange(len(raw)),\n",
    "    test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Train: {y_train_all.sum():,.0f} pions / {len(y_train_all):,} ({100*y_train_all.mean():.1f}%)\")\n",
    "print(f\"Test:  {y_test.sum():,.0f} pions / {len(y_test):,} ({100*y_test.mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_50k: 13,657 pions / 50,000 (27.3%)\n"
     ]
    }
   ],
   "source": [
    "idx = np.arange(len(y_train_all))\n",
    "_, idx_50k = train_test_split(\n",
    "    idx, test_size=TRAIN_50K, random_state=RANDOM_STATE, stratify=y_train_all\n",
    ")\n",
    "\n",
    "seq_train_50k  = seq_train_all[idx_50k]\n",
    "summ_train_50k = summ_train_all[idx_50k]\n",
    "mask_train_50k = mask_train_all[idx_50k]\n",
    "y_train_50k    = y_train_all[idx_50k]\n",
    "\n",
    "print(f\"train_50k: {y_train_50k.sum():,.0f} pions / {len(y_train_50k):,} ({100*y_train_50k.mean():.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq means:  dEdX=3.450, RR=33.580\n",
      "Seq stds:   dEdX=3.782,  RR=31.293\n",
      "Summ means: chi2=156.456, length=31.716, score=0.576, dEdX_med=0.458\n",
      "Summ stds:  chi2=121.441,  length=32.289, score=0.346, dEdX_med=0.976\n"
     ]
    }
   ],
   "source": [
    "def compute_norm_stats(seq_tr, mask_tr, summ_tr):\n",
    "    \"\"\"Compute clipping + z-score stats from training data only.\"\"\"\n",
    "    # Clip sequences\n",
    "    seq_tr = seq_tr.copy()\n",
    "    seq_tr[:, :, 0] = np.where(mask_tr, np.clip(seq_tr[:, :, 0], 0, DEDX_CLIP), 0)\n",
    "    seq_tr[:, :, 1] = np.where(mask_tr, np.clip(seq_tr[:, :, 1], 0, RR_CLIP),   0)\n",
    "    # Clip summary\n",
    "    summ_tr = summ_tr.copy()\n",
    "    summ_tr[:, 0] = np.clip(summ_tr[:, 0], 0, CHI2_CLIP)\n",
    "    summ_tr[:, 1] = np.clip(summ_tr[:, 1], 0, LEN_CLIP)\n",
    "    # Compute stats on real (non-padded) hits\n",
    "    train_hits = seq_tr[mask_tr.astype(bool)]\n",
    "    s_mean, s_std = train_hits.mean(0), train_hits.std(0)\n",
    "    sm_mean, sm_std = summ_tr.mean(0), summ_tr.std(0)\n",
    "    return s_mean, s_std, sm_mean, sm_std\n",
    "\n",
    "\n",
    "def apply_preprocessing(seq, summ, mask, s_mean, s_std, sm_mean, sm_std):\n",
    "    \"\"\"Apply clip + z-score normalisation using pre-computed stats.\"\"\"\n",
    "    seq = seq.copy()\n",
    "    seq[:, :, 0] = np.where(mask, np.clip(seq[:, :, 0], 0, DEDX_CLIP), 0)\n",
    "    seq[:, :, 1] = np.where(mask, np.clip(seq[:, :, 1], 0, RR_CLIP),   0)\n",
    "    summ = summ.copy()\n",
    "    summ[:, 0] = np.clip(summ[:, 0], 0, CHI2_CLIP)\n",
    "    summ[:, 1] = np.clip(summ[:, 1], 0, LEN_CLIP)\n",
    "    seq_n  = np.where(mask[:, :, None], (seq  - s_mean)  / s_std,  0.0).astype(np.float32)\n",
    "    summ_n = ((summ - sm_mean) / sm_std).astype(np.float32)\n",
    "    return seq_n, summ_n\n",
    "\n",
    "\n",
    "# Compute stats from train_all (the reference)\n",
    "s_mean, s_std, sm_mean, sm_std = compute_norm_stats(seq_train_all, mask_train_all, summ_train_all)\n",
    "print(f\"Seq means:  dEdX={s_mean[0]:.3f}, RR={s_mean[1]:.3f}\")\n",
    "print(f\"Seq stds:   dEdX={s_std[0]:.3f},  RR={s_std[1]:.3f}\")\n",
    "print(f\"Summ means: chi2={sm_mean[0]:.3f}, length={sm_mean[1]:.3f}, score={sm_mean[2]:.3f}, dEdX_med={sm_mean[3]:.3f}\")\n",
    "print(f\"Summ stds:  chi2={sm_std[0]:.3f},  length={sm_std[1]:.3f}, score={sm_std[2]:.3f}, dEdX_med={sm_std[3]:.3f}\")\n",
    "\n",
    "# Apply to all three splits\n",
    "seq_train_all_n,  summ_train_all_n  = apply_preprocessing(seq_train_all,  summ_train_all,  mask_train_all, s_mean, s_std, sm_mean, sm_std)\n",
    "seq_train_50k_n,  summ_train_50k_n  = apply_preprocessing(seq_train_50k,  summ_train_50k,  mask_train_50k, s_mean, s_std, sm_mean, sm_std)\n",
    "seq_test_n,       summ_test_n       = apply_preprocessing(seq_test,       summ_test,       mask_test,      s_mean, s_std, sm_mean, sm_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved prepared-data/train_all.pkl  (223,256 tracks)\n",
      "Saved prepared-data/train_50000.pkl  (50,000 tracks)\n",
      "Saved prepared-data/test.pkl  (55,815 tracks)\n",
      "Saved prepared-data/test_tracks.pkl  (55,815 tracks)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"prepared-data\", exist_ok=True)\n",
    "\n",
    "def save_split(path, sequences, summary, masks, labels):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump({\"sequences\": sequences, \"summary\": summary,\n",
    "                     \"masks\": masks, \"labels\": labels}, f)\n",
    "    print(f\"Saved {path}  ({sequences.shape[0]:,} tracks)\")\n",
    "\n",
    "save_split(\"prepared-data/train_all.pkl\",   seq_train_all_n, summ_train_all_n, mask_train_all, y_train_all)\n",
    "save_split(\"prepared-data/train_50000.pkl\", seq_train_50k_n, summ_train_50k_n, mask_train_50k, y_train_50k)\n",
    "save_split(\"prepared-data/test.pkl\",        seq_test_n,      summ_test_n,      mask_test,      y_test)\n",
    "\n",
    "# Save the full raw track dicts for the test split (for topology-level analysis)\n",
    "test_tracks = [raw[i] for i in idx_test]\n",
    "with open(\"prepared-data/test_tracks.pkl\", \"wb\") as f:\n",
    "    pickle.dump(test_tracks, f)\n",
    "print(f\"Saved prepared-data/test_tracks.pkl  ({len(test_tracks):,} tracks)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
